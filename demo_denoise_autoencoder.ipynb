{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision as tv\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision.utils import make_grid\n",
    "from art.attacks.evasion import BasicIterativeMethod\n",
    "from art.estimators.classification import PyTorchClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "from region_based_classifier import RegionBasedClassifier\n",
    "from util import dataset2tensor, get_correct_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device: {}'.format(device))\n",
    "\n",
    "n_threads = os.cpu_count()\n",
    "print('CPU threads: {}'.format(n_threads))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_DATA = 'data'\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 100\n",
    "NOISE_STRENGTH = 0.025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch dataset\n",
    "transforms = tv.transforms.Compose([tv.transforms.ToTensor()])\n",
    "dataset_train = datasets.MNIST(PATH_DATA, train=True, download=True, transform=transforms)\n",
    "dataset_test = datasets.MNIST(PATH_DATA, train=False, download=True, transform=transforms)\n",
    "\n",
    "# Split the origional training set into training set and validation set\n",
    "# From PyTorch dataset to Numpy array\n",
    "tensor_X_train, tensor_y_train = dataset2tensor(dataset_train)\n",
    "X_train = tensor_X_train.cpu().detach().numpy()\n",
    "y_train = tensor_y_train.cpu().detach().numpy()\n",
    "\n",
    "# Split model training set into training set and validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=5000)\n",
    "dataset_train = TensorDataset(\n",
    "    torch.tensor(X_train, dtype=torch.float32),\n",
    "    torch.tensor(y_train, dtype=torch.int64))\n",
    "dataset_val = TensorDataset(\n",
    "    torch.tensor(X_val, dtype=torch.float32),\n",
    "    torch.tensor(y_val, dtype=torch.int64))\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=BATCH_SIZE, shuffle=True)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "print(len(dataset_train), len(dataset_val), len(dataset_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Autoencoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_add_noise(X, x_min, x_max, epsilon, device):\n",
    "    \"\"\"Returns X with Gaussian noise and clip.\"\"\"\n",
    "    normal = torch.distributions.normal.Normal(\n",
    "        loc=torch.zeros(X.size(), dtype=torch.float32),\n",
    "        scale=1.0)\n",
    "    noise = normal.sample().to(device)\n",
    "    X_noisy = X + epsilon * noise\n",
    "    X_noisy = torch.clamp(X_noisy, x_min, x_max)\n",
    "    return X_noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder1(nn.Module):\n",
    "    def __init__(self, n_channel=1):\n",
    "        super(Autoencoder1, self).__init__()\n",
    "        self.n_channel = n_channel\n",
    "        self.conv1 = nn.Conv2d(self.n_channel, 3, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(3, 3, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(3, 3, 3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(3, 3, 3, padding=1)\n",
    "        self.conv5 = nn.Conv2d(3, self.n_channel, 3, padding=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.conv1(x))\n",
    "        x = F.avg_pool2d(x, kernel_size=2)\n",
    "        x = torch.sigmoid(self.conv2(x))\n",
    "        x = torch.sigmoid(self.conv3(x))\n",
    "        x = F.interpolate(x, scale_factor=2, mode='nearest')\n",
    "        x = torch.sigmoid(self.conv4(x))\n",
    "        x = torch.sigmoid(self.conv5(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae1 = Autoencoder1().to(device)\n",
    "ae1.eval()\n",
    "x, _ = next(iter(dataloader_test))\n",
    "print(x.size())\n",
    "x = x.to(device)\n",
    "output = ae1(x)\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder2(nn.Module):\n",
    "    def __init__(self, n_channel=1):\n",
    "        super(Autoencoder2, self).__init__()\n",
    "        self.n_channel = n_channel\n",
    "        self.conv1 = nn.Conv2d(self.n_channel, 3, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(3, 3, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(3, self.n_channel, 3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.sigmoid(self.conv1(x))\n",
    "        x = torch.sigmoid(self.conv2(x))\n",
    "        x = torch.sigmoid(self.conv3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae2 = Autoencoder2().to(device)\n",
    "ae2.eval()\n",
    "x, _ = next(iter(dataloader_test))\n",
    "print(x.size())\n",
    "x = x.to(device)\n",
    "output = ae1(x)\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_ae1 = optim.AdamW(ae1.parameters(), lr=0.001, weight_decay=1e-9)\n",
    "loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, loss, optimizer, device, noise_strength):\n",
    "    n = len(loader.dataset)\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for x, _ in loader:\n",
    "        x = x.to(device)\n",
    "        batch_size = x.size(0)\n",
    "        if noise_strength != 0:\n",
    "            x_noisy = torch_add_noise(x, 0.0, 1.0, noise_strength, device)\n",
    "        else:\n",
    "            x_noisy = x\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x_noisy)\n",
    "        l = loss(outputs, x)\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += l.item() * batch_size\n",
    "    total_loss = total_loss / float(n)\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, loader, loss, device):\n",
    "    n = len(loader.dataset)\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, _ in loader:\n",
    "            x = x.to(device)\n",
    "            batch_size = x.size(0)\n",
    "            outputs = model(x)\n",
    "            l = loss(outputs, x)\n",
    "            total_loss += l.item() * batch_size\n",
    "    total_loss = total_loss / float(n)\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(model, dataloader_train, dataloader_val, loss, optimizer, max_epochs, device):\n",
    "    history_tr_loss = np.zeros(max_epochs, dtype=np.float32) \n",
    "    history_va_loss = np.zeros(max_epochs, dtype=np.float32) \n",
    "\n",
    "    for epoch in tqdm(range(max_epochs)):\n",
    "        history_tr_loss[epoch] = train(model, dataloader_train, loss, optimizer, device, NOISE_STRENGTH)\n",
    "        history_va_loss[epoch] = validate(model, dataloader_val, loss, device)\n",
    "\n",
    "    return history_tr_loss, history_va_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train encoder\n",
    "tr_losses, va_losses = fit(ae1, dataloader_train, dataloader_val, loss, optimizer_ae1, EPOCHS, device)\n",
    "\n",
    "# Save results\n",
    "if not os.path.exists('results'):\n",
    "    os.makedirs('results')\n",
    "PATH_AUTOENCODER1 = os.path.join('results', 'ae1.pt')\n",
    "torch.save(ae1.state_dict(), PATH_AUTOENCODER1)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "x = np.arange(1, EPOCHS+1)\n",
    "plt.plot(x, tr_losses, c='blue', label='Training loss')\n",
    "plt.plot(x, va_losses, c='green', label='Validation loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results\n",
    "ae1.load_state_dict(torch.load(PATH_AUTOENCODER1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, _ = next(iter(dataloader_train))\n",
    "images = images[:16]\n",
    "images = images.to(device)\n",
    "\n",
    "images_noisy = torch_add_noise(images, 0.0, 1.0, NOISE_STRENGTH, device)\n",
    "\n",
    "ae1.eval()\n",
    "images_ae = ae1(images_noisy)\n",
    "\n",
    "grid_image = make_grid(images).permute(1, 2, 0).cpu().detach().numpy()\n",
    "grid_image_noisy = make_grid(images_noisy).permute(1, 2, 0).cpu().detach().numpy()\n",
    "grid_image_ae = make_grid(images_ae).permute(1, 2, 0).cpu().detach().numpy()\n",
    "\n",
    "plt.figure(figsize=(14, 4))\n",
    "plt.imshow(grid_image)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(14, 4))\n",
    "plt.imshow(grid_image_noisy)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(14, 4))\n",
    "plt.imshow(grid_image_ae)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python36964bit4ada1bcfcd0f4caa95e946f583001e1b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}