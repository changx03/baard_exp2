{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision as tv\n",
    "import torchvision.datasets as datasets\n",
    "from art.attacks.evasion import FastGradientMethod\n",
    "from art.estimators.classification import PyTorchClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from util import get_correct_examples, dataset2tensor, generate_random_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "CPU threads: 24\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device: {}'.format(device))\n",
    "\n",
    "n_threads = os.cpu_count()\n",
    "print('CPU threads: {}'.format(n_threads))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'data'\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 28, 28])\n",
      "torch.Size([10000, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# Fetch dataset\n",
    "transforms = tv.transforms.Compose([tv.transforms.ToTensor()])\n",
    "dataset_train = datasets.MNIST(PATH, train=True, download=True, transform=transforms)\n",
    "dataset_test = datasets.MNIST(PATH, train=False, download=True, transform=transforms)\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "print(dataset_train.data.size())\n",
    "print(dataset_test.data.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train point-based classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers: 10\n"
     ]
    }
   ],
   "source": [
    "# Create Neural Network model\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(1, 32, 3, 1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(32, 64, 3, 1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(9216, 200),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(200, 10),\n",
    "    nn.LogSoftmax(dim=1)\n",
    ")\n",
    "model.to(device)\n",
    "print('Number of layers: {}'.format(len(list(model.children()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, loss, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    corrects = 0.\n",
    "    \n",
    "    for x, y in loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x)\n",
    "        l = loss(outputs, y)\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # for display\n",
    "        total_loss += l.item() * batch_size\n",
    "        preds = outputs.max(1, keepdim=True)[1]\n",
    "        corrects += preds.eq(y.view_as(preds)).sum().item()\n",
    "    \n",
    "    n = len(loader.dataset)\n",
    "    total_loss = total_loss / n\n",
    "    accuracy = corrects / n\n",
    "    return total_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, loader, loss, device=device):\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    corrects = 0.\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            batch_size = x.size(0)\n",
    "            outputs = model(x)\n",
    "            l = loss(outputs, y)\n",
    "            total_loss += l.item() * batch_size\n",
    "            preds = outputs.max(1, keepdim=True)[1]\n",
    "            corrects += preds.eq(y.view_as(preds)).sum().item()\n",
    "    \n",
    "    n = len(loader.dataset)\n",
    "    total_loss = total_loss / n\n",
    "    accuracy = corrects / n\n",
    "    return total_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, X, batch_size=BATCH_SIZE, device=device):\n",
    "    model.eval()\n",
    "    dataset = TensorDataset(X)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    predictions = -torch.ones(len(X), dtype=torch.long)\n",
    "    \n",
    "    start = 0\n",
    "    with torch.no_grad():\n",
    "        for x in loader:\n",
    "            x = x[0].to(device)\n",
    "            n = x.size(0)\n",
    "            end = start + n\n",
    "            outputs = model(x)\n",
    "            preds = outputs.max(1)[1].type(torch.long)\n",
    "            predictions[start:end] = preds\n",
    "            start += n\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "loss=nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1] 0m 4.4s Train Loss: 0.4088 Accuracy: 88.2917%, Test Loss: 0.1413 Accuracy: 95.5000%\n",
      "[ 2] 0m 4.3s Train Loss: 0.1241 Accuracy: 96.1700%, Test Loss: 0.0939 Accuracy: 96.9800%\n",
      "[ 3] 0m 4.3s Train Loss: 0.0811 Accuracy: 97.4700%, Test Loss: 0.0709 Accuracy: 97.8700%\n",
      "[ 4] 0m 4.4s Train Loss: 0.0579 Accuracy: 98.2267%, Test Loss: 0.0536 Accuracy: 98.2200%\n",
      "[ 5] 0m 4.3s Train Loss: 0.0440 Accuracy: 98.6150%, Test Loss: 0.0422 Accuracy: 98.6300%\n",
      "Total run time: 0m 21.7s\n"
     ]
    }
   ],
   "source": [
    "since = time.time()\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    tr_loss, tr_acc = train(model, dataloader_train, loss, optimizer)\n",
    "    va_loss, va_acc = validate(model, dataloader_test, loss)\n",
    "    \n",
    "    time_elapsed = time.time() - start\n",
    "    print(('[{:2d}] {:.0f}m {:.1f}s Train Loss: {:.4f} Accuracy: {:.4f}%, ' +\n",
    "        'Test Loss: {:.4f} Accuracy: {:.4f}%').format(\n",
    "            epoch+1, time_elapsed // 60, time_elapsed % 60,\n",
    "            tr_loss, tr_acc*100.,\n",
    "            va_loss, va_acc*100.))\n",
    "    \n",
    "time_elapsed = time.time() - since\n",
    "print('Total run time: {:.0f}m {:.1f}s'.format(\n",
    "    time_elapsed // 60,\n",
    "    time_elapsed % 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 60000\n",
      "Accuracy on 59508 filtered training examples: 100.0000%\n",
      "Test set: 10000\n",
      "Accuracy on 9863 filtered test examples: 100.0000%\n"
     ]
    }
   ],
   "source": [
    "print('Training set: {}'.format(len(dataset_train)))\n",
    "tensor_train_X, tensor_train_y = get_correct_examples(model, dataset_train, device=device, return_tensor=True)\n",
    "dataset_train_perfect = TensorDataset(tensor_train_X, tensor_train_y)\n",
    "dataloader_train_perfect = DataLoader(dataset_train_perfect, batch_size=512, shuffle=True)\n",
    "_, acc = validate(model, dataloader_train_perfect, loss)\n",
    "print('Accuracy on {} filtered training examples: {:.4f}%'.format(len(dataloader_train_perfect.dataset), acc*100))\n",
    "\n",
    "print('Test set: {}'.format(len(dataset_test)))\n",
    "tensor_test_X, tensor_test_y = get_correct_examples(model, dataset_test, device=device, return_tensor=True)\n",
    "dataset_test_perfect = TensorDataset(tensor_test_X, tensor_test_y)\n",
    "dataloader_test_perfect = DataLoader(dataset_test_perfect, batch_size=512, shuffle=True)\n",
    "_, acc = validate(model, dataloader_test_perfect, loss)\n",
    "print('Accuracy on {} filtered test examples: {:.4f}%'.format(len(dataloader_test_perfect.dataset), acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ADV = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = PyTorchClassifier(\n",
    "    model=model, \n",
    "    loss=loss, \n",
    "    input_shape=(1, 28, 28), \n",
    "    optimizer=optimizer,\n",
    "    nb_classes=10,\n",
    "    clip_values=(0.0, 1.0),\n",
    "    device_type=device\n",
    ")\n",
    "\n",
    "attack = FastGradientMethod(estimator=classifier, eps=0.2)\n",
    "# attack = BasicIterativeMethod(estimator=classifier, eps=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset for adversarial examples\n",
    "n = len(dataset_test_perfect)\n",
    "indices = torch.randperm(n)[:N_ADV]\n",
    "\n",
    "pt_subset_X = tensor_test_X[indices]  # PyTorch Tensor\n",
    "pt_subset_y = tensor_test_y[indices]\n",
    "\n",
    "subset_X = pt_subset_X.cpu().detach().numpy()\n",
    "subset_y = pt_subset_y.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy on clean examples: 100.0000%\n",
      "Model accuracy on adversarial examples: 38.4000%\n"
     ]
    }
   ],
   "source": [
    "# Create adversarial examples\n",
    "subset_pred = np.argmax(classifier.predict(subset_X), axis=1)\n",
    "accuracy = np.sum(subset_pred == subset_y) / float(len(subset_pred))\n",
    "print(\"Model accuracy on clean examples: {:.4f}%\".format(accuracy * 100))\n",
    "\n",
    "# Generate adversarial examples\n",
    "subset_adv = attack.generate(x=subset_X)\n",
    "subset_pred = np.argmax(classifier.predict(subset_adv), axis=1)\n",
    "\n",
    "accuracy = np.sum(subset_pred == subset_y) / float(len(subset_pred))\n",
    "print(\"Model accuracy on adversarial examples: {:.4f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Region-based classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55000, 1, 28, 28)\n",
      "(5000, 1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# From PyTorch dataset to Numpy array\n",
    "pt_X_train, pt_y_train = dataset2tensor(dataset_train)\n",
    "X_train = pt_X_train.cpu().detach().numpy()\n",
    "y_train = pt_y_train.cpu().detach().numpy()\n",
    "\n",
    "# Split model training set into training set and validation set\n",
    "X_rc_train, X_rc_val, y_rc_train, y_rc_val = train_test_split(X_train, y_train, test_size=5000)\n",
    "\n",
    "print(X_rc_train.shape)\n",
    "print(X_rc_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "X_rng = generate_random_samples(X_rc_val[0], clip_values=(0.0, 1.0), r=0.3, size=1000)\n",
    "\n",
    "print(X_rng.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n"
     ]
    }
   ],
   "source": [
    "pt_preds_rng = predict(model, torch.tensor(X_rng, dtype=torch.float32))\n",
    "preds_rng = pt_preds_rng.cpu().detach().numpy()\n",
    "pred = np.bincount(preds_rng).argmax()\n",
    "\n",
    "print(pred, y_rc_val[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python36964bit4ada1bcfcd0f4caa95e946f583001e1b"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
