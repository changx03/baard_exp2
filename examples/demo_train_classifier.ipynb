{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from defences.util import get_range, normalize\n",
    "from pt_dataset import PTDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data from CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data: (5500, 41)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A1</th>\n",
       "      <th>A2</th>\n",
       "      <th>A3</th>\n",
       "      <th>A4</th>\n",
       "      <th>A5</th>\n",
       "      <th>A6</th>\n",
       "      <th>A7</th>\n",
       "      <th>A8</th>\n",
       "      <th>A9</th>\n",
       "      <th>A10</th>\n",
       "      <th>...</th>\n",
       "      <th>A32</th>\n",
       "      <th>A33</th>\n",
       "      <th>A34</th>\n",
       "      <th>A35</th>\n",
       "      <th>A36</th>\n",
       "      <th>A37</th>\n",
       "      <th>A38</th>\n",
       "      <th>A39</th>\n",
       "      <th>A40</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.223</td>\n",
       "      <td>-0.798</td>\n",
       "      <td>-0.867</td>\n",
       "      <td>-0.639</td>\n",
       "      <td>-0.545</td>\n",
       "      <td>-0.412</td>\n",
       "      <td>-0.795</td>\n",
       "      <td>-0.629</td>\n",
       "      <td>-0.547</td>\n",
       "      <td>-0.868</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.766</td>\n",
       "      <td>-0.555</td>\n",
       "      <td>-0.714</td>\n",
       "      <td>-0.545</td>\n",
       "      <td>-0.587</td>\n",
       "      <td>-0.871</td>\n",
       "      <td>-0.620</td>\n",
       "      <td>-0.568</td>\n",
       "      <td>-0.607</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.410</td>\n",
       "      <td>-1.029</td>\n",
       "      <td>-1.013</td>\n",
       "      <td>-0.895</td>\n",
       "      <td>-0.762</td>\n",
       "      <td>-0.676</td>\n",
       "      <td>-1.043</td>\n",
       "      <td>-0.851</td>\n",
       "      <td>-0.775</td>\n",
       "      <td>-1.037</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.919</td>\n",
       "      <td>-0.770</td>\n",
       "      <td>-0.847</td>\n",
       "      <td>-0.663</td>\n",
       "      <td>-0.723</td>\n",
       "      <td>-1.013</td>\n",
       "      <td>-0.748</td>\n",
       "      <td>-0.698</td>\n",
       "      <td>-0.817</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.107</td>\n",
       "      <td>-0.649</td>\n",
       "      <td>-0.629</td>\n",
       "      <td>-0.492</td>\n",
       "      <td>-0.367</td>\n",
       "      <td>-0.298</td>\n",
       "      <td>-0.682</td>\n",
       "      <td>-0.478</td>\n",
       "      <td>-0.395</td>\n",
       "      <td>-0.681</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.692</td>\n",
       "      <td>-0.445</td>\n",
       "      <td>-0.588</td>\n",
       "      <td>-0.371</td>\n",
       "      <td>-0.368</td>\n",
       "      <td>-0.746</td>\n",
       "      <td>-0.457</td>\n",
       "      <td>-0.379</td>\n",
       "      <td>-0.469</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.270</td>\n",
       "      <td>-0.855</td>\n",
       "      <td>-0.958</td>\n",
       "      <td>-0.707</td>\n",
       "      <td>-0.619</td>\n",
       "      <td>-0.469</td>\n",
       "      <td>-0.872</td>\n",
       "      <td>-0.705</td>\n",
       "      <td>-0.620</td>\n",
       "      <td>-0.988</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.829</td>\n",
       "      <td>-0.719</td>\n",
       "      <td>-0.774</td>\n",
       "      <td>-0.617</td>\n",
       "      <td>-0.688</td>\n",
       "      <td>-0.937</td>\n",
       "      <td>-0.693</td>\n",
       "      <td>-0.657</td>\n",
       "      <td>-0.779</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.331</td>\n",
       "      <td>-0.862</td>\n",
       "      <td>-0.761</td>\n",
       "      <td>-0.689</td>\n",
       "      <td>-0.498</td>\n",
       "      <td>-0.361</td>\n",
       "      <td>-0.857</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>-0.496</td>\n",
       "      <td>-0.779</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.861</td>\n",
       "      <td>-0.571</td>\n",
       "      <td>-0.784</td>\n",
       "      <td>-0.545</td>\n",
       "      <td>-0.562</td>\n",
       "      <td>-0.952</td>\n",
       "      <td>-0.642</td>\n",
       "      <td>-0.578</td>\n",
       "      <td>-0.648</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      A1     A2     A3     A4     A5     A6     A7     A8     A9    A10  ...  \\\n",
       "0 -1.223 -0.798 -0.867 -0.639 -0.545 -0.412 -0.795 -0.629 -0.547 -0.868  ...   \n",
       "1 -1.410 -1.029 -1.013 -0.895 -0.762 -0.676 -1.043 -0.851 -0.775 -1.037  ...   \n",
       "2 -1.107 -0.649 -0.629 -0.492 -0.367 -0.298 -0.682 -0.478 -0.395 -0.681  ...   \n",
       "3 -1.270 -0.855 -0.958 -0.707 -0.619 -0.469 -0.872 -0.705 -0.620 -0.988  ...   \n",
       "4 -1.331 -0.862 -0.761 -0.689 -0.498 -0.361 -0.857 -0.600 -0.496 -0.779  ...   \n",
       "\n",
       "     A32    A33    A34    A35    A36    A37    A38    A39    A40  Class  \n",
       "0 -0.766 -0.555 -0.714 -0.545 -0.587 -0.871 -0.620 -0.568 -0.607      0  \n",
       "1 -0.919 -0.770 -0.847 -0.663 -0.723 -1.013 -0.748 -0.698 -0.817      0  \n",
       "2 -0.692 -0.445 -0.588 -0.371 -0.368 -0.746 -0.457 -0.379 -0.469      0  \n",
       "3 -0.829 -0.719 -0.774 -0.617 -0.688 -0.937 -0.693 -0.657 -0.779      0  \n",
       "4 -0.861 -0.571 -0.784 -0.545 -0.562 -0.952 -0.642 -0.578 -0.648      0  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_INDEX = 3\n",
    "files = [\n",
    "    os.path.join('..', 'data', 'banknote_preprocessed.csv'),\n",
    "    os.path.join('..', 'data', 'htru2_preprocessed.csv'),\n",
    "    os.path.join('..', 'data', 'segment_preprocessed.csv'),\n",
    "    os.path.join('..', 'data', 'texture_preprocessed.csv'),\n",
    "]\n",
    "file_path = files[DATA_INDEX]\n",
    "df = pd.read_csv(file_path, sep=',')\n",
    "\n",
    "print('Data:', df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (5500, 40)\n",
      "y: (5500,)\n",
      "features: 40, classes: 11\n"
     ]
    }
   ],
   "source": [
    "y = df['Class'].to_numpy().astype(np.long)\n",
    "X = df.drop(['Class'], axis=1).to_numpy().astype(np.float32)\n",
    "n_features = X.shape[1]\n",
    "n_classes = len(np.unique(y))\n",
    "print('X:', X.shape)\n",
    "print('y:', y.shape)\n",
    "print('features: {}, classes: {}'.format(n_features, n_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_test: 600\n",
      "X_train (4900, 40)\n"
     ]
    }
   ],
   "source": [
    "# Split data\n",
    "# For Banknote, Yeast, Segment, uses 400 test examples\n",
    "# For Abalone, Texture, uses 600 test examples\n",
    "# For htru2, uses 4000 test examples\n",
    "if re.search(r'(banknote)|(yeast)|(segment)', file_path):\n",
    "    N_TEST = 400\n",
    "elif re.search(r'(abalone)|(texture)', file_path):\n",
    "    N_TEST = 600\n",
    "else:\n",
    "    N_TEST = 4000\n",
    "print('n_test:', N_TEST)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=N_TEST)\n",
    "print('X_train', X_train.shape)\n",
    "\n",
    "# Apply scaling\n",
    "xmin, xmax = get_range(X_train)\n",
    "X_train = normalize(X_train, xmin, xmax)\n",
    "X_test = normalize(X_test, xmin, xmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a PyTorch Nueral Network classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "EPOCHS = 500\n",
    "if re.search('htru2', file_path):\n",
    "    EPOCHS = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = PTDataset(X_train, y_train)\n",
    "dataset_test = PTDataset(X_test, y_test)\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "dataloader_test = DataLoader(dataset_test, BATCH_SIZE, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, loss, optimizer, ):\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    corrects = 0.\n",
    "    \n",
    "    for x, y in loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x)\n",
    "        l = loss(output, y)\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # for display\n",
    "        total_loss += l.item() * batch_size\n",
    "        preds = output.max(1, keepdim=True)[1]\n",
    "        corrects += preds.eq(y.view_as(preds)).sum().item()\n",
    "    \n",
    "    n = len(loader.dataset)\n",
    "    total_loss = total_loss / n\n",
    "    accuracy = corrects / n\n",
    "    return total_loss, accuracy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, loader, loss):\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    corrects = 0.\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            batch_size = x.size(0)\n",
    "            output = model(x)\n",
    "            l = loss(output, y)\n",
    "            total_loss += l.item() * batch_size\n",
    "            preds = output.max(1, keepdim=True)[1]\n",
    "            corrects += preds.eq(y.view_as(preds)).sum().item()\n",
    "    \n",
    "    n = len(loader.dataset)\n",
    "    total_loss = total_loss / n\n",
    "    accuracy = corrects / n\n",
    "    return total_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=40, out_features=160, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=160, out_features=160, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=160, out_features=11, bias=True)\n",
       "  (5): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "            nn.Linear(n_features, n_features*4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_features*4, n_features*4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_features*4, n_classes),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "# Banknote dataset works better on SGD\n",
    "if re.search('banknote', file_path):\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "loss = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1] 0m 0.1s Train Loss: 2.3966 Accuracy: 14.4082%, Test Loss: 2.3951 Accuracy: 17.8333%\n",
      "[ 11] 0m 0.1s Train Loss: 2.2243 Accuracy: 36.6327%, Test Loss: 2.2207 Accuracy: 34.1667%\n",
      "[ 21] 0m 0.1s Train Loss: 1.9856 Accuracy: 64.8367%, Test Loss: 1.9928 Accuracy: 64.0000%\n",
      "[ 31] 0m 0.1s Train Loss: 1.8870 Accuracy: 74.3265%, Test Loss: 1.9004 Accuracy: 72.6667%\n",
      "[ 41] 0m 0.1s Train Loss: 1.8263 Accuracy: 79.2857%, Test Loss: 1.8398 Accuracy: 78.1667%\n",
      "[ 51] 0m 0.1s Train Loss: 1.7802 Accuracy: 81.5306%, Test Loss: 1.7932 Accuracy: 80.6667%\n",
      "[ 61] 0m 0.1s Train Loss: 1.7521 Accuracy: 83.0612%, Test Loss: 1.7659 Accuracy: 82.3333%\n",
      "[ 71] 0m 0.1s Train Loss: 1.7336 Accuracy: 84.1429%, Test Loss: 1.7452 Accuracy: 83.5000%\n",
      "[ 81] 0m 0.1s Train Loss: 1.7189 Accuracy: 85.2449%, Test Loss: 1.7307 Accuracy: 84.5000%\n",
      "[ 91] 0m 0.1s Train Loss: 1.7074 Accuracy: 86.0612%, Test Loss: 1.7198 Accuracy: 84.8333%\n",
      "[101] 0m 0.1s Train Loss: 1.6983 Accuracy: 86.5306%, Test Loss: 1.7118 Accuracy: 84.8333%\n",
      "[111] 0m 0.1s Train Loss: 1.6909 Accuracy: 87.1633%, Test Loss: 1.7059 Accuracy: 85.5000%\n",
      "[121] 0m 0.1s Train Loss: 1.6853 Accuracy: 87.4082%, Test Loss: 1.6985 Accuracy: 85.6667%\n",
      "[131] 0m 0.1s Train Loss: 1.6811 Accuracy: 87.6939%, Test Loss: 1.6946 Accuracy: 85.6667%\n",
      "[141] 0m 0.1s Train Loss: 1.6769 Accuracy: 88.0000%, Test Loss: 1.6904 Accuracy: 86.0000%\n",
      "[151] 0m 0.1s Train Loss: 1.6736 Accuracy: 88.1429%, Test Loss: 1.6866 Accuracy: 86.5000%\n",
      "[161] 0m 0.1s Train Loss: 1.6711 Accuracy: 88.2245%, Test Loss: 1.6855 Accuracy: 86.8333%\n",
      "[171] 0m 0.1s Train Loss: 1.6686 Accuracy: 88.5306%, Test Loss: 1.6818 Accuracy: 87.0000%\n",
      "[181] 0m 0.1s Train Loss: 1.6662 Accuracy: 88.5714%, Test Loss: 1.6785 Accuracy: 87.5000%\n",
      "[191] 0m 0.1s Train Loss: 1.6642 Accuracy: 88.6735%, Test Loss: 1.6777 Accuracy: 87.1667%\n",
      "[201] 0m 0.1s Train Loss: 1.6624 Accuracy: 88.7959%, Test Loss: 1.6748 Accuracy: 87.5000%\n",
      "[211] 0m 0.1s Train Loss: 1.6227 Accuracy: 95.2041%, Test Loss: 1.6209 Accuracy: 95.3333%\n",
      "[221] 0m 0.1s Train Loss: 1.6114 Accuracy: 95.7347%, Test Loss: 1.6090 Accuracy: 95.6667%\n",
      "[231] 0m 0.1s Train Loss: 1.6058 Accuracy: 95.8571%, Test Loss: 1.6018 Accuracy: 96.1667%\n",
      "[241] 0m 0.1s Train Loss: 1.6010 Accuracy: 96.1224%, Test Loss: 1.5968 Accuracy: 96.5000%\n",
      "[251] 0m 0.1s Train Loss: 1.5980 Accuracy: 96.2857%, Test Loss: 1.5936 Accuracy: 96.5000%\n",
      "[261] 0m 0.1s Train Loss: 1.5937 Accuracy: 96.4694%, Test Loss: 1.5923 Accuracy: 96.5000%\n",
      "[271] 0m 0.1s Train Loss: 1.5910 Accuracy: 96.6531%, Test Loss: 1.5889 Accuracy: 97.0000%\n",
      "[281] 0m 0.1s Train Loss: 1.5876 Accuracy: 96.8571%, Test Loss: 1.5848 Accuracy: 97.3333%\n",
      "[291] 0m 0.1s Train Loss: 1.5856 Accuracy: 97.0408%, Test Loss: 1.5833 Accuracy: 97.3333%\n",
      "[301] 0m 0.1s Train Loss: 1.5835 Accuracy: 97.2041%, Test Loss: 1.5813 Accuracy: 97.3333%\n",
      "[311] 0m 0.1s Train Loss: 1.5816 Accuracy: 97.3878%, Test Loss: 1.5809 Accuracy: 97.3333%\n",
      "[321] 0m 0.1s Train Loss: 1.5795 Accuracy: 97.5918%, Test Loss: 1.5778 Accuracy: 97.3333%\n",
      "[331] 0m 0.1s Train Loss: 1.5781 Accuracy: 97.5714%, Test Loss: 1.5777 Accuracy: 97.3333%\n",
      "[341] 0m 0.1s Train Loss: 1.5765 Accuracy: 97.6939%, Test Loss: 1.5766 Accuracy: 97.5000%\n",
      "[351] 0m 0.1s Train Loss: 1.5749 Accuracy: 97.8367%, Test Loss: 1.5748 Accuracy: 97.3333%\n",
      "[361] 0m 0.1s Train Loss: 1.5738 Accuracy: 97.9592%, Test Loss: 1.5732 Accuracy: 97.8333%\n",
      "[371] 0m 0.1s Train Loss: 1.5726 Accuracy: 97.9796%, Test Loss: 1.5725 Accuracy: 97.8333%\n",
      "[381] 0m 0.1s Train Loss: 1.5714 Accuracy: 98.0816%, Test Loss: 1.5702 Accuracy: 98.1667%\n",
      "[391] 0m 0.1s Train Loss: 1.5706 Accuracy: 98.1429%, Test Loss: 1.5700 Accuracy: 97.8333%\n",
      "[401] 0m 0.1s Train Loss: 1.5699 Accuracy: 98.2041%, Test Loss: 1.5685 Accuracy: 97.8333%\n",
      "[411] 0m 0.1s Train Loss: 1.5684 Accuracy: 98.3061%, Test Loss: 1.5671 Accuracy: 98.1667%\n",
      "[421] 0m 0.1s Train Loss: 1.5679 Accuracy: 98.3061%, Test Loss: 1.5670 Accuracy: 98.1667%\n",
      "[431] 0m 0.1s Train Loss: 1.5669 Accuracy: 98.4490%, Test Loss: 1.5657 Accuracy: 98.3333%\n",
      "[441] 0m 0.1s Train Loss: 1.5663 Accuracy: 98.4490%, Test Loss: 1.5648 Accuracy: 98.6667%\n",
      "[451] 0m 0.1s Train Loss: 1.5653 Accuracy: 98.5102%, Test Loss: 1.5634 Accuracy: 98.8333%\n",
      "[461] 0m 0.1s Train Loss: 1.5649 Accuracy: 98.4082%, Test Loss: 1.5626 Accuracy: 98.8333%\n",
      "[471] 0m 0.1s Train Loss: 1.5641 Accuracy: 98.5306%, Test Loss: 1.5620 Accuracy: 98.8333%\n",
      "[481] 0m 0.1s Train Loss: 1.5634 Accuracy: 98.6735%, Test Loss: 1.5615 Accuracy: 98.8333%\n",
      "[491] 0m 0.1s Train Loss: 1.5629 Accuracy: 98.6735%, Test Loss: 1.5599 Accuracy: 99.0000%\n",
      "Total run time: 0m 34.1s\n"
     ]
    }
   ],
   "source": [
    "since = time.time()\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    tr_loss, tr_acc = train(model, dataloader_train, loss, optimizer)\n",
    "    va_loss, va_acc = validate(model, dataloader_test, loss)\n",
    "    \n",
    "    time_elapsed = time.time() - start\n",
    "    if epoch % 10 == 0:\n",
    "        print(('[{:3d}] {:.0f}m {:.1f}s Train Loss: {:.4f} Accuracy: {:.4f}%, ' +\n",
    "            'Test Loss: {:.4f} Accuracy: {:.4f}%').format(\n",
    "                epoch+1, time_elapsed // 60, time_elapsed % 60,\n",
    "                tr_loss, tr_acc*100.,\n",
    "                va_loss, va_acc*100.))\n",
    "    \n",
    "time_elapsed = time.time() - since\n",
    "print('Total run time: {:.0f}m {:.1f}s'.format(\n",
    "    time_elapsed // 60,\n",
    "    time_elapsed % 60))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9933333333333333"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test on SVM\n",
    "model2 = SVC()\n",
    "model2.fit(X_train, y_train)\n",
    "model2.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=40, out_features=160, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=160, out_features=160, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=160, out_features=11, bias=True)\n",
       "  (5): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Sequential(*list(model.children()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=40, out_features=160, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=160, out_features=160, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=160, out_features=11, bias=True)\n",
       "  (5): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Sequential(*list(model.children()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_features, n_features*4)\n",
    "        self.fc2 = nn.Linear(n_features*4, n_features*4)\n",
    "        self.fc3 = nn.Linear(n_features*4, n_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Classifier(\n",
       "  (fc1): Linear(in_features=40, out_features=160, bias=True)\n",
       "  (fc2): Linear(in_features=160, out_features=160, bias=True)\n",
       "  (fc3): Linear(in_features=160, out_features=11, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3 = Classifier()\n",
    "model3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Linear(in_features=40, out_features=160, bias=True),\n",
       " Linear(in_features=160, out_features=160, bias=True),\n",
       " Linear(in_features=160, out_features=11, bias=True)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model3.children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
