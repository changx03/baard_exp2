{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from util import get_range, normalize\n",
    "from pt_dataset import PTDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data from CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Data: (5500, 41)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      A1     A2     A3     A4     A5     A6     A7     A8     A9    A10  ...  \\\n",
       "0 -1.223 -0.798 -0.867 -0.639 -0.545 -0.412 -0.795 -0.629 -0.547 -0.868  ...   \n",
       "1 -1.410 -1.029 -1.013 -0.895 -0.762 -0.676 -1.043 -0.851 -0.775 -1.037  ...   \n",
       "2 -1.107 -0.649 -0.629 -0.492 -0.367 -0.298 -0.682 -0.478 -0.395 -0.681  ...   \n",
       "3 -1.270 -0.855 -0.958 -0.707 -0.619 -0.469 -0.872 -0.705 -0.620 -0.988  ...   \n",
       "4 -1.331 -0.862 -0.761 -0.689 -0.498 -0.361 -0.857 -0.600 -0.496 -0.779  ...   \n",
       "\n",
       "     A32    A33    A34    A35    A36    A37    A38    A39    A40  Class  \n",
       "0 -0.766 -0.555 -0.714 -0.545 -0.587 -0.871 -0.620 -0.568 -0.607      0  \n",
       "1 -0.919 -0.770 -0.847 -0.663 -0.723 -1.013 -0.748 -0.698 -0.817      0  \n",
       "2 -0.692 -0.445 -0.588 -0.371 -0.368 -0.746 -0.457 -0.379 -0.469      0  \n",
       "3 -0.829 -0.719 -0.774 -0.617 -0.688 -0.937 -0.693 -0.657 -0.779      0  \n",
       "4 -0.861 -0.571 -0.784 -0.545 -0.562 -0.952 -0.642 -0.578 -0.648      0  \n",
       "\n",
       "[5 rows x 41 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>A1</th>\n      <th>A2</th>\n      <th>A3</th>\n      <th>A4</th>\n      <th>A5</th>\n      <th>A6</th>\n      <th>A7</th>\n      <th>A8</th>\n      <th>A9</th>\n      <th>A10</th>\n      <th>...</th>\n      <th>A32</th>\n      <th>A33</th>\n      <th>A34</th>\n      <th>A35</th>\n      <th>A36</th>\n      <th>A37</th>\n      <th>A38</th>\n      <th>A39</th>\n      <th>A40</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-1.223</td>\n      <td>-0.798</td>\n      <td>-0.867</td>\n      <td>-0.639</td>\n      <td>-0.545</td>\n      <td>-0.412</td>\n      <td>-0.795</td>\n      <td>-0.629</td>\n      <td>-0.547</td>\n      <td>-0.868</td>\n      <td>...</td>\n      <td>-0.766</td>\n      <td>-0.555</td>\n      <td>-0.714</td>\n      <td>-0.545</td>\n      <td>-0.587</td>\n      <td>-0.871</td>\n      <td>-0.620</td>\n      <td>-0.568</td>\n      <td>-0.607</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-1.410</td>\n      <td>-1.029</td>\n      <td>-1.013</td>\n      <td>-0.895</td>\n      <td>-0.762</td>\n      <td>-0.676</td>\n      <td>-1.043</td>\n      <td>-0.851</td>\n      <td>-0.775</td>\n      <td>-1.037</td>\n      <td>...</td>\n      <td>-0.919</td>\n      <td>-0.770</td>\n      <td>-0.847</td>\n      <td>-0.663</td>\n      <td>-0.723</td>\n      <td>-1.013</td>\n      <td>-0.748</td>\n      <td>-0.698</td>\n      <td>-0.817</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-1.107</td>\n      <td>-0.649</td>\n      <td>-0.629</td>\n      <td>-0.492</td>\n      <td>-0.367</td>\n      <td>-0.298</td>\n      <td>-0.682</td>\n      <td>-0.478</td>\n      <td>-0.395</td>\n      <td>-0.681</td>\n      <td>...</td>\n      <td>-0.692</td>\n      <td>-0.445</td>\n      <td>-0.588</td>\n      <td>-0.371</td>\n      <td>-0.368</td>\n      <td>-0.746</td>\n      <td>-0.457</td>\n      <td>-0.379</td>\n      <td>-0.469</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-1.270</td>\n      <td>-0.855</td>\n      <td>-0.958</td>\n      <td>-0.707</td>\n      <td>-0.619</td>\n      <td>-0.469</td>\n      <td>-0.872</td>\n      <td>-0.705</td>\n      <td>-0.620</td>\n      <td>-0.988</td>\n      <td>...</td>\n      <td>-0.829</td>\n      <td>-0.719</td>\n      <td>-0.774</td>\n      <td>-0.617</td>\n      <td>-0.688</td>\n      <td>-0.937</td>\n      <td>-0.693</td>\n      <td>-0.657</td>\n      <td>-0.779</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-1.331</td>\n      <td>-0.862</td>\n      <td>-0.761</td>\n      <td>-0.689</td>\n      <td>-0.498</td>\n      <td>-0.361</td>\n      <td>-0.857</td>\n      <td>-0.600</td>\n      <td>-0.496</td>\n      <td>-0.779</td>\n      <td>...</td>\n      <td>-0.861</td>\n      <td>-0.571</td>\n      <td>-0.784</td>\n      <td>-0.545</td>\n      <td>-0.562</td>\n      <td>-0.952</td>\n      <td>-0.642</td>\n      <td>-0.578</td>\n      <td>-0.648</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 41 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "DATA_INDEX = 3\n",
    "files = [\n",
    "    os.path.join('..', 'data', 'banknote_preprocessed.csv'),\n",
    "    os.path.join('..', 'data', 'htru2_preprocessed.csv'),\n",
    "    os.path.join('..', 'data', 'segment_preprocessed.csv'),\n",
    "    os.path.join('..', 'data', 'texture_preprocessed.csv'),\n",
    "]\n",
    "file_path = files[DATA_INDEX]\n",
    "df = pd.read_csv(file_path, sep=',')\n",
    "\n",
    "print('Data:', df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "X: (5500, 40)\ny: (5500,)\nfeatures: 40, classes: 11\n"
     ]
    }
   ],
   "source": [
    "y = df['Class'].to_numpy().astype(np.long)\n",
    "X = df.drop(['Class'], axis=1).to_numpy().astype(np.float32)\n",
    "n_features = X.shape[1]\n",
    "n_classes = len(np.unique(y))\n",
    "print('X:', X.shape)\n",
    "print('y:', y.shape)\n",
    "print('features: {}, classes: {}'.format(n_features, n_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "n_test: 600\nX_train (4900, 40)\n"
     ]
    }
   ],
   "source": [
    "# Split data\n",
    "# For Banknote, Yeast, Segment, uses 400 test examples\n",
    "# For Abalone, Texture, uses 600 test examples\n",
    "# For htru2, uses 4000 test examples\n",
    "if re.search(r'(banknote)|(yeast)|(segment)', file_path):\n",
    "    N_TEST = 400\n",
    "elif re.search(r'(abalone)|(texture)', file_path):\n",
    "    N_TEST = 600\n",
    "else:\n",
    "    N_TEST = 4000\n",
    "print('n_test:', N_TEST)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=N_TEST)\n",
    "print('X_train', X_train.shape)\n",
    "\n",
    "# Apply scaling\n",
    "xmin, xmax = get_range(X_train)\n",
    "X_train = normalize(X_train, xmin, xmax)\n",
    "X_test = normalize(X_test, xmin, xmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a PyTorch Nueral Network classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "EPOCHS = 500\n",
    "if re.search('htru2', file_path):\n",
    "    EPOCHS = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = PTDataset(X_train, y_train)\n",
    "dataset_test = PTDataset(X_test, y_test)\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "dataloader_test = DataLoader(dataset_test, BATCH_SIZE, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, loss, optimizer, ):\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    corrects = 0.\n",
    "    \n",
    "    for x, y in loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x)\n",
    "        l = loss(output, y)\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # for display\n",
    "        total_loss += l.item() * batch_size\n",
    "        preds = output.max(1, keepdim=True)[1]\n",
    "        corrects += preds.eq(y.view_as(preds)).sum().item()\n",
    "    \n",
    "    n = len(loader.dataset)\n",
    "    total_loss = total_loss / n\n",
    "    accuracy = corrects / n\n",
    "    return total_loss, accuracy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, loader, loss):\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    corrects = 0.\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            batch_size = x.size(0)\n",
    "            output = model(x)\n",
    "            l = loss(output, y)\n",
    "            total_loss += l.item() * batch_size\n",
    "            preds = output.max(1, keepdim=True)[1]\n",
    "            corrects += preds.eq(y.view_as(preds)).sum().item()\n",
    "    \n",
    "    n = len(loader.dataset)\n",
    "    total_loss = total_loss / n\n",
    "    accuracy = corrects / n\n",
    "    return total_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=40, out_features=160, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=160, out_features=160, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=160, out_features=11, bias=True)\n",
       "  (5): Softmax(dim=1)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "            nn.Linear(n_features, n_features*4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_features*4, n_features*4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_features*4, n_classes),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "# Banknote dataset works better on SGD\n",
    "if re.search('banknote', file_path):\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "loss = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[  1] 0m 0.1s Train Loss: 2.3961 Accuracy: 8.4898%, Test Loss: 2.3946 Accuracy: 9.6667%\n",
      "[ 11] 0m 0.1s Train Loss: 2.2491 Accuracy: 29.6735%, Test Loss: 2.2274 Accuracy: 30.1667%\n",
      "[ 21] 0m 0.1s Train Loss: 2.0070 Accuracy: 63.8776%, Test Loss: 1.9767 Accuracy: 67.3333%\n",
      "[ 31] 0m 0.1s Train Loss: 1.9020 Accuracy: 69.6735%, Test Loss: 1.8799 Accuracy: 71.0000%\n",
      "[ 41] 0m 0.1s Train Loss: 1.8425 Accuracy: 78.0000%, Test Loss: 1.8238 Accuracy: 79.5000%\n",
      "[ 51] 0m 0.1s Train Loss: 1.7914 Accuracy: 81.3061%, Test Loss: 1.7772 Accuracy: 81.3333%\n",
      "[ 61] 0m 0.1s Train Loss: 1.7421 Accuracy: 87.3265%, Test Loss: 1.7359 Accuracy: 86.6667%\n",
      "[ 71] 0m 0.1s Train Loss: 1.7143 Accuracy: 88.7551%, Test Loss: 1.7116 Accuracy: 88.3333%\n",
      "[ 81] 0m 0.1s Train Loss: 1.6938 Accuracy: 89.8776%, Test Loss: 1.6926 Accuracy: 89.3333%\n",
      "[ 91] 0m 0.1s Train Loss: 1.6775 Accuracy: 90.9388%, Test Loss: 1.6766 Accuracy: 90.5000%\n",
      "[101] 0m 0.1s Train Loss: 1.6638 Accuracy: 91.8163%, Test Loss: 1.6633 Accuracy: 91.6667%\n",
      "[111] 0m 0.1s Train Loss: 1.6520 Accuracy: 92.6939%, Test Loss: 1.6556 Accuracy: 92.1667%\n",
      "[121] 0m 0.1s Train Loss: 1.6428 Accuracy: 93.3061%, Test Loss: 1.6443 Accuracy: 93.3333%\n",
      "[131] 0m 0.1s Train Loss: 1.6336 Accuracy: 94.0204%, Test Loss: 1.6355 Accuracy: 93.8333%\n",
      "[141] 0m 0.1s Train Loss: 1.6266 Accuracy: 94.6122%, Test Loss: 1.6275 Accuracy: 94.8333%\n",
      "[151] 0m 0.1s Train Loss: 1.6202 Accuracy: 94.7347%, Test Loss: 1.6204 Accuracy: 95.0000%\n",
      "[161] 0m 0.1s Train Loss: 1.6156 Accuracy: 95.0816%, Test Loss: 1.6170 Accuracy: 94.6667%\n",
      "[171] 0m 0.1s Train Loss: 1.6118 Accuracy: 95.2041%, Test Loss: 1.6122 Accuracy: 95.1667%\n",
      "[181] 0m 0.1s Train Loss: 1.6074 Accuracy: 95.5714%, Test Loss: 1.6092 Accuracy: 95.3333%\n",
      "[191] 0m 0.1s Train Loss: 1.6041 Accuracy: 95.7959%, Test Loss: 1.6047 Accuracy: 95.3333%\n",
      "[201] 0m 0.1s Train Loss: 1.6009 Accuracy: 95.9796%, Test Loss: 1.6008 Accuracy: 96.0000%\n",
      "[211] 0m 0.1s Train Loss: 1.5981 Accuracy: 96.1633%, Test Loss: 1.5983 Accuracy: 95.8333%\n",
      "[221] 0m 0.1s Train Loss: 1.5958 Accuracy: 96.2449%, Test Loss: 1.5966 Accuracy: 96.1667%\n",
      "[231] 0m 0.1s Train Loss: 1.5935 Accuracy: 96.3469%, Test Loss: 1.5937 Accuracy: 96.0000%\n",
      "[241] 0m 0.1s Train Loss: 1.5912 Accuracy: 96.5510%, Test Loss: 1.5921 Accuracy: 96.1667%\n",
      "[251] 0m 0.1s Train Loss: 1.5897 Accuracy: 96.5510%, Test Loss: 1.5900 Accuracy: 96.6667%\n",
      "[261] 0m 0.1s Train Loss: 1.5876 Accuracy: 96.8163%, Test Loss: 1.5869 Accuracy: 96.5000%\n",
      "[271] 0m 0.1s Train Loss: 1.5860 Accuracy: 96.9796%, Test Loss: 1.5873 Accuracy: 96.5000%\n",
      "[281] 0m 0.1s Train Loss: 1.5847 Accuracy: 96.9388%, Test Loss: 1.5853 Accuracy: 96.8333%\n",
      "[291] 0m 0.1s Train Loss: 1.5834 Accuracy: 96.9592%, Test Loss: 1.5832 Accuracy: 96.8333%\n",
      "[301] 0m 0.1s Train Loss: 1.5819 Accuracy: 97.1020%, Test Loss: 1.5843 Accuracy: 97.0000%\n",
      "[311] 0m 0.1s Train Loss: 1.5808 Accuracy: 97.2245%, Test Loss: 1.5815 Accuracy: 96.8333%\n",
      "[321] 0m 0.1s Train Loss: 1.5788 Accuracy: 97.3469%, Test Loss: 1.5811 Accuracy: 96.6667%\n",
      "[331] 0m 0.1s Train Loss: 1.5779 Accuracy: 97.3469%, Test Loss: 1.5805 Accuracy: 96.8333%\n",
      "[341] 0m 0.1s Train Loss: 1.5769 Accuracy: 97.5102%, Test Loss: 1.5803 Accuracy: 97.0000%\n",
      "[351] 0m 0.1s Train Loss: 1.5752 Accuracy: 97.7143%, Test Loss: 1.5791 Accuracy: 97.0000%\n",
      "[361] 0m 0.1s Train Loss: 1.5744 Accuracy: 97.7551%, Test Loss: 1.5780 Accuracy: 96.8333%\n",
      "[371] 0m 0.1s Train Loss: 1.5731 Accuracy: 97.8980%, Test Loss: 1.5768 Accuracy: 97.0000%\n",
      "[381] 0m 0.1s Train Loss: 1.5720 Accuracy: 98.0000%, Test Loss: 1.5757 Accuracy: 97.1667%\n",
      "[391] 0m 0.1s Train Loss: 1.5710 Accuracy: 98.0204%, Test Loss: 1.5756 Accuracy: 97.1667%\n",
      "[401] 0m 0.1s Train Loss: 1.5700 Accuracy: 98.1020%, Test Loss: 1.5739 Accuracy: 97.0000%\n",
      "[411] 0m 0.1s Train Loss: 1.5696 Accuracy: 98.1429%, Test Loss: 1.5727 Accuracy: 97.1667%\n",
      "[421] 0m 0.1s Train Loss: 1.5690 Accuracy: 98.1837%, Test Loss: 1.5717 Accuracy: 97.3333%\n",
      "[431] 0m 0.1s Train Loss: 1.5678 Accuracy: 98.2449%, Test Loss: 1.5719 Accuracy: 97.3333%\n",
      "[441] 0m 0.1s Train Loss: 1.5673 Accuracy: 98.2041%, Test Loss: 1.5711 Accuracy: 97.5000%\n",
      "[451] 0m 0.1s Train Loss: 1.5668 Accuracy: 98.3265%, Test Loss: 1.5706 Accuracy: 97.8333%\n",
      "[461] 0m 0.1s Train Loss: 1.5654 Accuracy: 98.4490%, Test Loss: 1.5699 Accuracy: 97.5000%\n",
      "[471] 0m 0.1s Train Loss: 1.5647 Accuracy: 98.5306%, Test Loss: 1.5701 Accuracy: 97.6667%\n",
      "[481] 0m 0.1s Train Loss: 1.5642 Accuracy: 98.6122%, Test Loss: 1.5688 Accuracy: 97.8333%\n",
      "[491] 0m 0.1s Train Loss: 1.5638 Accuracy: 98.6531%, Test Loss: 1.5684 Accuracy: 97.6667%\n",
      "Total run time: 0m 35.9s\n"
     ]
    }
   ],
   "source": [
    "since = time.time()\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    tr_loss, tr_acc = train(model, dataloader_train, loss, optimizer)\n",
    "    va_loss, va_acc = validate(model, dataloader_test, loss)\n",
    "    \n",
    "    time_elapsed = time.time() - start\n",
    "    if epoch % 10 == 0:\n",
    "        print(('[{:3d}] {:.0f}m {:.1f}s Train Loss: {:.4f} Accuracy: {:.4f}%, ' +\n",
    "            'Test Loss: {:.4f} Accuracy: {:.4f}%').format(\n",
    "                epoch+1, time_elapsed // 60, time_elapsed % 60,\n",
    "                tr_loss, tr_acc*100.,\n",
    "                va_loss, va_acc*100.))\n",
    "    \n",
    "time_elapsed = time.time() - since\n",
    "print('Total run time: {:.0f}m {:.1f}s'.format(\n",
    "    time_elapsed // 60,\n",
    "    time_elapsed % 60))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.9933333333333333"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "# Test on SVM\n",
    "model2 = SVC()\n",
    "model2.fit(X_train, y_train)\n",
    "model2.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=40, out_features=160, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=160, out_features=160, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=160, out_features=11, bias=True)\n",
       "  (5): Softmax(dim=1)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "nn.Sequential(*list(model.children()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=40, out_features=160, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=160, out_features=160, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=160, out_features=11, bias=True)\n",
       "  (5): Softmax(dim=1)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "nn.Sequential(*list(model.children()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_features, n_features*4)\n",
    "        self.fc2 = nn.Linear(n_features*4, n_features*4)\n",
    "        self.fc3 = nn.Linear(n_features*4, n_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Classifier(\n",
       "  (fc1): Linear(in_features=40, out_features=160, bias=True)\n",
       "  (fc2): Linear(in_features=160, out_features=160, bias=True)\n",
       "  (fc3): Linear(in_features=160, out_features=11, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "model3 = Classifier()\n",
    "model3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[Linear(in_features=40, out_features=160, bias=True),\n",
       " Linear(in_features=160, out_features=160, bias=True),\n",
       " Linear(in_features=160, out_features=11, bias=True)]"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "list(model3.children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}