{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import scale, MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torchvision as tv\n",
    "import torchvision.datasets as datasets\n",
    "import art\n",
    "\n",
    "from util import get_roc, get_shape, get_correct_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "CPU threads: 6\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device: {}'.format(device))\n",
    "\n",
    "n_threads = os.cpu_count()\n",
    "print('CPU threads: {}'.format(n_threads))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'data'\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch dataset\n",
    "transforms = tv.transforms.Compose([tv.transforms.ToTensor()])\n",
    "dataset_train = datasets.MNIST(PATH, train=True, download=True, transform=transforms)\n",
    "dataset_test = datasets.MNIST(PATH, train=False, download=True, transform=transforms)\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (1): ReLU()\n",
       "  (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (3): ReLU()\n",
       "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (5): Flatten(start_dim=1, end_dim=-1)\n",
       "  (6): Linear(in_features=9216, out_features=200, bias=True)\n",
       "  (7): ReLU()\n",
       "  (8): Linear(in_features=200, out_features=10, bias=True)\n",
       "  (9): LogSoftmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Neural Network model\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(1, 32, 3, 1),\n",
    "    nn.ReLU(),\n",
    "    nn.Conv2d(32, 64, 3, 1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(9216, 200),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(200, 10),\n",
    "    nn.LogSoftmax(dim=1)\n",
    ")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, loss, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    corrects = 0.\n",
    "    \n",
    "    for x, y in loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x)\n",
    "        l = loss(outputs, y)\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # for display\n",
    "        total_loss += l.item() * batch_size\n",
    "        preds = outputs.max(1, keepdim=True)[1]\n",
    "        corrects += preds.eq(y.view_as(preds)).sum().item()\n",
    "    \n",
    "    n = len(loader.dataset)\n",
    "    total_loss = total_loss / n\n",
    "    accuracy = corrects / n\n",
    "    return total_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, loader, loss):\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    corrects = 0.\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            batch_size = x.size(0)\n",
    "            outputs = model(x)\n",
    "            l = loss(outputs, y)\n",
    "            total_loss += l.item() * batch_size\n",
    "            preds = outputs.max(1, keepdim=True)[1]\n",
    "            corrects += preds.eq(y.view_as(preds)).sum().item()\n",
    "    \n",
    "    n = len(loader.dataset)\n",
    "    total_loss = total_loss / n\n",
    "    accuracy = corrects / n\n",
    "    return total_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "loss=nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1] 0m 6.3s Train Loss: 0.4175 Accuracy: 87.5767%, Test Loss: 0.1569 Accuracy: 94.9400%\n",
      "[ 2] 0m 6.2s Train Loss: 0.1272 Accuracy: 96.0500%, Test Loss: 0.1009 Accuracy: 96.8800%\n",
      "[ 3] 0m 6.2s Train Loss: 0.0812 Accuracy: 97.5150%, Test Loss: 0.0846 Accuracy: 97.4500%\n",
      "[ 4] 0m 6.2s Train Loss: 0.0575 Accuracy: 98.2283%, Test Loss: 0.0527 Accuracy: 98.2500%\n",
      "[ 5] 0m 6.2s Train Loss: 0.0433 Accuracy: 98.6867%, Test Loss: 0.0519 Accuracy: 98.3200%\n",
      "[ 6] 0m 6.2s Train Loss: 0.0333 Accuracy: 98.9800%, Test Loss: 0.0465 Accuracy: 98.4500%\n",
      "[ 7] 0m 6.2s Train Loss: 0.0261 Accuracy: 99.1900%, Test Loss: 0.0428 Accuracy: 98.5900%\n",
      "[ 8] 0m 6.2s Train Loss: 0.0208 Accuracy: 99.3567%, Test Loss: 0.0412 Accuracy: 98.6800%\n",
      "[ 9] 0m 6.2s Train Loss: 0.0163 Accuracy: 99.4900%, Test Loss: 0.0375 Accuracy: 98.8200%\n",
      "[10] 0m 6.2s Train Loss: 0.0148 Accuracy: 99.5417%, Test Loss: 0.0404 Accuracy: 98.7000%\n",
      "[11] 0m 6.2s Train Loss: 0.0107 Accuracy: 99.6767%, Test Loss: 0.0408 Accuracy: 98.6900%\n",
      "[12] 0m 6.2s Train Loss: 0.0081 Accuracy: 99.7950%, Test Loss: 0.0457 Accuracy: 98.5600%\n",
      "[13] 0m 6.2s Train Loss: 0.0068 Accuracy: 99.8150%, Test Loss: 0.0471 Accuracy: 98.8700%\n",
      "[14] 0m 6.1s Train Loss: 0.0050 Accuracy: 99.8733%, Test Loss: 0.0479 Accuracy: 98.7100%\n",
      "[15] 0m 6.1s Train Loss: 0.0043 Accuracy: 99.8967%, Test Loss: 0.0414 Accuracy: 98.7800%\n",
      "[16] 0m 6.2s Train Loss: 0.0027 Accuracy: 99.9567%, Test Loss: 0.0403 Accuracy: 98.8800%\n",
      "[17] 0m 6.1s Train Loss: 0.0018 Accuracy: 99.9700%, Test Loss: 0.0448 Accuracy: 98.8600%\n",
      "[18] 0m 6.2s Train Loss: 0.0017 Accuracy: 99.9617%, Test Loss: 0.0406 Accuracy: 98.9000%\n",
      "[19] 0m 6.2s Train Loss: 0.0010 Accuracy: 99.9883%, Test Loss: 0.0427 Accuracy: 98.8500%\n",
      "[20] 0m 6.2s Train Loss: 0.0007 Accuracy: 99.9900%, Test Loss: 0.0435 Accuracy: 98.8600%\n",
      "Total run time: 2m 3.7s\n"
     ]
    }
   ],
   "source": [
    "since = time.time()\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    tr_loss, tr_acc = train(model, dataloader_train, loss, optimizer)\n",
    "    va_loss, va_acc = validate(model, dataloader_test, loss)\n",
    "    \n",
    "    time_elapsed = time.time() - start\n",
    "    print(('[{:2d}] {:.0f}m {:.1f}s Train Loss: {:.4f} Accuracy: {:.4f}%, ' +\n",
    "        'Test Loss: {:.4f} Accuracy: {:.4f}%').format(\n",
    "            epoch+1, time_elapsed // 60, time_elapsed % 60,\n",
    "            tr_loss, tr_acc*100.,\n",
    "            va_loss, va_acc*100.))\n",
    "    \n",
    "time_elapsed = time.time() - since\n",
    "print('Total run time: {:.0f}m {:.1f}s'.format(\n",
    "    time_elapsed // 60,\n",
    "    time_elapsed % 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.0000%\n"
     ]
    }
   ],
   "source": [
    "dataset_perfect = get_correct_examples(model, dataset_train, device=device)\n",
    "dataloader_perfect = DataLoader(dataset_perfect, batch_size=512, shuffle=True)\n",
    "_, acc = validate(model, dataloader_perfect, loss)\n",
    "print('Accuracy: {:.4f}%'.format(acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
