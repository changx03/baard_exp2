{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing texture dataset\n",
    "\n",
    "The samples with label 0 cannot be recognised by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from defences.util import dataset2tensor\n",
    "from models.numeric import NumericModel\n",
    "from models.torch_util import train, validate, print_acc_per_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_FILE = 'texture_preprocessed.csv'\n",
    "TEST_SIZE = 1000\n",
    "# DATASET_FILE = 'segment_preprocessed.csv'\n",
    "TEST_SIZE = 400\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join('..', 'data', DATASET_FILE)\n",
    "df = pd.read_csv(data_path, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(5000, 40) (5000,)\n[0 1 2 3 4 5 6 7 8 9]\n10\n"
     ]
    }
   ],
   "source": [
    "y = df['Class'].to_numpy().astype(np.long)\n",
    "X = df.drop(['Class'], axis=1).to_numpy().astype(np.float32)\n",
    "labels = np.unique(y)\n",
    "\n",
    "N_FEATURES = X.shape[1]\n",
    "N_CLASSES = len(labels)\n",
    "\n",
    "print(X.shape, y.shape)\n",
    "print(labels)\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[-1.226, -0.648, -0.503, -0.473, -0.256, -0.138, -0.7  , -0.424,\n",
       "        -0.319, -0.647, -1.226, -0.717, -0.48 , -0.679, -0.456, -0.533,\n",
       "        -0.873, -0.555, -0.529, -0.611, -1.226, -0.939, -0.853, -0.822,\n",
       "        -0.678, -0.592, -0.943, -0.76 , -0.689, -0.88 , -1.226, -0.835,\n",
       "        -0.633, -0.791, -0.586, -0.632, -0.958, -0.689, -0.643, -0.725],\n",
       "       [-1.299, -0.613, -0.595, -0.466, -0.275, -0.205, -0.748, -0.497,\n",
       "        -0.42 , -0.866, -1.299, -0.728, -0.562, -0.692, -0.475, -0.571,\n",
       "        -0.884, -0.571, -0.531, -0.659, -1.299, -1.075, -0.98 , -0.985,\n",
       "        -0.839, -0.767, -1.08 , -0.906, -0.848, -0.99 , -1.299, -0.925,\n",
       "        -0.814, -0.849, -0.727, -0.731, -0.994, -0.802, -0.728, -0.813],\n",
       "       [-1.072, -0.479, -0.563, -0.27 , -0.174, -0.097, -0.482, -0.343,\n",
       "        -0.256, -0.689, -1.072, -0.618, -0.454, -0.588, -0.409, -0.486,\n",
       "        -0.753, -0.494, -0.447, -0.505, -1.072, -0.815, -0.861, -0.695,\n",
       "        -0.636, -0.563, -0.8  , -0.691, -0.637, -0.858, -1.072, -0.593,\n",
       "        -0.54 , -0.588, -0.489, -0.625, -0.728, -0.518, -0.489, -0.564],\n",
       "       [-1.153, -0.553, -0.614, -0.401, -0.274, -0.165, -0.656, -0.467,\n",
       "        -0.362, -0.856, -1.153, -0.587, -0.388, -0.621, -0.434, -0.519,\n",
       "        -0.817, -0.509, -0.468, -0.491, -1.153, -0.866, -0.846, -0.751,\n",
       "        -0.647, -0.574, -0.866, -0.713, -0.649, -0.847, -1.153, -0.786,\n",
       "        -0.739, -0.744, -0.636, -0.688, -0.907, -0.708, -0.642, -0.753],\n",
       "       [-1.228, -0.724, -0.605, -0.567, -0.387, -0.275, -0.8  , -0.565,\n",
       "        -0.454, -0.753, -1.228, -0.806, -0.593, -0.774, -0.57 , -0.597,\n",
       "        -0.97 , -0.687, -0.619, -0.712, -1.228, -1.015, -0.963, -0.912,\n",
       "        -0.806, -0.747, -0.992, -0.861, -0.81 , -0.955, -1.228, -0.914,\n",
       "        -0.769, -0.87 , -0.715, -0.697, -1.022, -0.811, -0.735, -0.825]],\n",
       "      dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "X[:5]"
   ]
  },
  {
   "source": [
    "## ISSUE\n",
    "\n",
    "Normalization is the problem"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Normalize data\n",
    "standard = StandardScaler().fit(X)\n",
    "X = standard.transform(X)\n",
    "\n",
    "# minmax = MinMaxScaler(feature_range=(0,1)).fit(X)\n",
    "# X = minmax.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[-1.6991761 -3.0204427 -2.3236039 -3.1316085 -3.0885038 -3.4629464\n -3.0620043 -2.9431925 -3.1191134 -2.0393856 -1.6991761 -2.4841971\n -2.1920342 -2.925627  -2.6142213 -2.629669  -2.3299778 -2.34724\n -2.448813  -2.0833259 -1.6991761 -2.2112603 -1.7178668 -2.2588022\n -2.4808836 -2.794671  -2.2396243 -2.3203485 -2.4681873 -1.9936692\n -1.6991761 -2.782059  -1.921552  -3.267038  -2.6351414 -2.4603403\n -2.8487718 -2.5013092 -2.6669505 -2.040127 ]\n[8.943331  4.362412  2.8368363 4.2319946 3.15971   3.0317202 5.533038\n 3.6416948 3.1631396 2.7966745 8.943331  4.6503043 2.9794364 5.627714\n 3.0580568 3.747291  3.7859569 2.5217977 2.3836923 2.558026  8.943331\n 5.8147793 3.557258  4.6076035 3.4804652 2.9618707 5.694679  3.9676657\n 3.4285564 3.646693  8.943331  6.221525  3.3817766 6.460301  4.1681867\n 4.2579293 5.1361303 3.4013631 3.4550242 3.195457 ]\n[-3.0517577e-09 -1.2207031e-08  6.1035155e-09  0.0000000e+00\n  0.0000000e+00 -2.4414062e-08  2.4414062e-08  1.2207031e-08\n  1.2207031e-08 -6.1035155e-09 -3.0517577e-09 -2.1362304e-08\n  1.2207031e-08  2.4414062e-08  0.0000000e+00  1.2207031e-08\n  6.1035155e-09 -2.4414062e-08  0.0000000e+00  6.1035155e-09\n -3.0517577e-09  1.3732910e-08  0.0000000e+00  6.1035155e-09\n -2.4414062e-08  6.1035155e-09 -1.5258790e-08  0.0000000e+00\n  1.2207031e-08 -4.8828124e-08 -3.0517577e-09  0.0000000e+00\n  1.2207031e-08 -2.4414062e-08 -2.4414062e-08  1.2207031e-08\n  0.0000000e+00  1.2207031e-08  2.4414062e-08  2.4414062e-08]\n"
     ]
    }
   ],
   "source": [
    "print(X.min(axis=0))\n",
    "print(X.max(axis=0))\n",
    "print(X.mean(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(4600, 40) (400, 40)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=1234)\n",
    "\n",
    "print(X_train.shape, X_test.shape)\n",
    "\n",
    "dataset_train = TensorDataset(torch.from_numpy(X_train).type(torch.float32), torch.from_numpy(y_train).type(torch.long))\n",
    "dataset_test = TensorDataset(torch.from_numpy(X_train).type(torch.float32), torch.from_numpy(y_train).type(torch.long))\n",
    "loader_train = DataLoader(dataset_train, BATCH_SIZE, shuffle=True)\n",
    "loader_test = DataLoader(dataset_test, BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NumericModel(N_FEATURES, N_FEATURES * 4, N_CLASSES, use_prob=True).to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS, eta_min=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "10/400[0:00:00.061324] Train Loss: 2.0805 Acc: 40.3478%, Test Loss: 2.0643 Acc: 40.3478%\n",
      "20/400[0:00:00.071826] Train Loss: 1.9097 Acc: 60.3913%, Test Loss: 1.9022 Acc: 60.8043%\n",
      "30/400[0:00:00.062217] Train Loss: 1.7677 Acc: 76.9130%, Test Loss: 1.7611 Acc: 77.8261%\n",
      "40/400[0:00:00.057972] Train Loss: 1.6585 Acc: 85.7609%, Test Loss: 1.6539 Acc: 85.7826%\n",
      "50/400[0:00:00.059918] Train Loss: 1.5764 Acc: 93.7826%, Test Loss: 1.5721 Acc: 94.0217%\n",
      "60/400[0:00:00.060584] Train Loss: 1.5214 Acc: 97.4130%, Test Loss: 1.5195 Acc: 97.5435%\n",
      "70/400[0:00:00.058187] Train Loss: 1.4997 Acc: 98.4348%, Test Loss: 1.4990 Acc: 98.4130%\n",
      "80/400[0:00:00.059341] Train Loss: 1.4903 Acc: 98.6522%, Test Loss: 1.4898 Acc: 98.7174%\n",
      "90/400[0:00:00.059783] Train Loss: 1.4851 Acc: 98.8913%, Test Loss: 1.4848 Acc: 98.9130%\n",
      "100/400[0:00:00.094333] Train Loss: 1.4816 Acc: 99.0870%, Test Loss: 1.4814 Acc: 99.0870%\n",
      "110/400[0:00:00.059701] Train Loss: 1.4792 Acc: 99.2391%, Test Loss: 1.4790 Acc: 99.2391%\n",
      "120/400[0:00:00.059618] Train Loss: 1.4773 Acc: 99.3478%, Test Loss: 1.4772 Acc: 99.3261%\n",
      "130/400[0:00:00.055623] Train Loss: 1.4759 Acc: 99.3696%, Test Loss: 1.4757 Acc: 99.3913%\n",
      "140/400[0:00:00.054928] Train Loss: 1.4748 Acc: 99.4565%, Test Loss: 1.4746 Acc: 99.4783%\n",
      "150/400[0:00:00.056434] Train Loss: 1.4737 Acc: 99.5217%, Test Loss: 1.4736 Acc: 99.5217%\n",
      "160/400[0:00:00.055539] Train Loss: 1.4729 Acc: 99.5217%, Test Loss: 1.4728 Acc: 99.5217%\n",
      "170/400[0:00:00.057261] Train Loss: 1.4722 Acc: 99.5435%, Test Loss: 1.4721 Acc: 99.5435%\n",
      "180/400[0:00:00.055060] Train Loss: 1.4716 Acc: 99.5435%, Test Loss: 1.4715 Acc: 99.5435%\n",
      "190/400[0:00:00.054770] Train Loss: 1.4711 Acc: 99.5435%, Test Loss: 1.4710 Acc: 99.5435%\n",
      "200/400[0:00:00.055623] Train Loss: 1.4706 Acc: 99.5870%, Test Loss: 1.4706 Acc: 99.5870%\n",
      "210/400[0:00:00.056527] Train Loss: 1.4703 Acc: 99.6087%, Test Loss: 1.4702 Acc: 99.5870%\n",
      "220/400[0:00:00.055579] Train Loss: 1.4699 Acc: 99.6304%, Test Loss: 1.4698 Acc: 99.6522%\n",
      "230/400[0:00:00.087013] Train Loss: 1.4695 Acc: 99.6957%, Test Loss: 1.4695 Acc: 99.6957%\n",
      "240/400[0:00:00.055094] Train Loss: 1.4693 Acc: 99.6957%, Test Loss: 1.4692 Acc: 99.6957%\n",
      "250/400[0:00:00.054641] Train Loss: 1.4691 Acc: 99.6957%, Test Loss: 1.4690 Acc: 99.6957%\n",
      "260/400[0:00:00.057296] Train Loss: 1.4689 Acc: 99.7174%, Test Loss: 1.4688 Acc: 99.7174%\n",
      "270/400[0:00:00.056882] Train Loss: 1.4687 Acc: 99.7174%, Test Loss: 1.4687 Acc: 99.7174%\n",
      "280/400[0:00:00.055669] Train Loss: 1.4686 Acc: 99.7174%, Test Loss: 1.4686 Acc: 99.7174%\n",
      "290/400[0:00:00.055963] Train Loss: 1.4685 Acc: 99.7174%, Test Loss: 1.4684 Acc: 99.7174%\n",
      "300/400[0:00:00.055876] Train Loss: 1.4684 Acc: 99.7174%, Test Loss: 1.4684 Acc: 99.7391%\n",
      "310/400[0:00:00.059639] Train Loss: 1.4683 Acc: 99.7391%, Test Loss: 1.4683 Acc: 99.7391%\n",
      "320/400[0:00:00.055504] Train Loss: 1.4682 Acc: 99.7609%, Test Loss: 1.4682 Acc: 99.7609%\n",
      "330/400[0:00:00.055119] Train Loss: 1.4682 Acc: 99.7609%, Test Loss: 1.4682 Acc: 99.7609%\n",
      "340/400[0:00:00.055579] Train Loss: 1.4681 Acc: 99.7609%, Test Loss: 1.4681 Acc: 99.7609%\n",
      "350/400[0:00:00.055139] Train Loss: 1.4681 Acc: 99.7609%, Test Loss: 1.4681 Acc: 99.7609%\n",
      "360/400[0:00:00.088454] Train Loss: 1.4681 Acc: 99.7609%, Test Loss: 1.4681 Acc: 99.7609%\n",
      "370/400[0:00:00.057626] Train Loss: 1.4681 Acc: 99.7609%, Test Loss: 1.4681 Acc: 99.7609%\n",
      "380/400[0:00:00.057854] Train Loss: 1.4681 Acc: 99.7609%, Test Loss: 1.4681 Acc: 99.7609%\n",
      "390/400[0:00:00.055741] Train Loss: 1.4681 Acc: 99.7609%, Test Loss: 1.4681 Acc: 99.7609%\n",
      "400/400[0:00:00.057780] Train Loss: 1.4681 Acc: 99.7609%, Test Loss: 1.4681 Acc: 99.7609%\n",
      "Total run time: 0:00:23.708964\n"
     ]
    }
   ],
   "source": [
    "since = time.time()\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    tr_loss, tr_acc = train(model, loader_train, loss, optimizer, device)\n",
    "    va_loss, va_acc = validate(model, loader_test, loss, device)\n",
    "    scheduler.step()\n",
    "\n",
    "    time_elapsed = time.time() - start\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print('{:2d}/{:d}[{:s}] Train Loss: {:.4f} Acc: {:.4f}%, Test Loss: {:.4f} Acc: {:.4f}%'.format(\n",
    "            epoch+1, EPOCHS, str(datetime.timedelta(seconds=time_elapsed)), tr_loss, tr_acc*100, va_loss, va_acc*100))\n",
    "\n",
    "time_elapsed = time.time() - since\n",
    "print('Total run time:', str(datetime.timedelta(seconds=time_elapsed)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, X, device):\n",
    "    model.eval()\n",
    "    dataset = TensorDataset(torch.from_numpy(X))\n",
    "    loader = DataLoader(dataset, batch_size=512, shuffle=False)\n",
    "    tensor_pred = -torch.ones(len(X), dtype=torch.long)\n",
    "    \n",
    "    start = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            x = batch[0].to(device)\n",
    "            end = start + x.size(0)\n",
    "            outputs = model(x)\n",
    "            tensor_pred[start:end] = outputs.max(1)[1].type(torch.long).cpu().detach()\n",
    "            start = end\n",
    "    return tensor_pred.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training set:\n[0] 455/455\n[1] 470/472\n[2] 453/454\n[3] 465/467\n[4] 447/447\n[5] 446/448\n[6] 465/466\n[7] 467/467\n[8] 469/471\n[9] 452/453\n"
     ]
    }
   ],
   "source": [
    "print('Training set:')\n",
    "X, y = dataset2tensor(dataset_train)\n",
    "X = X.cpu().detach().numpy()\n",
    "y = y.cpu().detach().numpy()\n",
    "print_acc_per_label(model, X, y, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Test set:\n",
      "[0] 455/455\n",
      "[1] 470/472\n",
      "[2] 453/454\n",
      "[3] 465/467\n",
      "[4] 447/447\n",
      "[5] 446/448\n",
      "[6] 465/466\n",
      "[7] 467/467\n",
      "[8] 469/471\n",
      "[9] 452/453\n"
     ]
    }
   ],
   "source": [
    "print('Test set:')\n",
    "X, y = dataset2tensor(dataset_test)\n",
    "X = X.cpu().detach().numpy()\n",
    "y = y.cpu().detach().numpy()\n",
    "print_acc_per_label(model, X, y, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}